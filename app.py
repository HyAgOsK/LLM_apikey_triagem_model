import streamlit as st
import os
import google.generativeai as genai


# App title
st.set_page_config(page_title="Triagem Chat üìç", page_icon=":heavy_heart_exclamation_mark_ornament:")


# Initialize Gemini-Pro 
genai.configure(api_key="AIzaSyDvhexcZ8XV2qwMpDUHslpXCv0Tb6urrJI")
generation_config = {
    "candidate_count": 1,  # N√∫mero de sugest√µes a serem geradas
    "temperature": 0.5,   # N√≠vel de criatividade (0 = mais conservador, 1 = mais criativo)
}

safety_settings = {
    'HATE': 'BLOCK_NONE',
    'HARASSMENT': 'BLOCK_NONE',
    'SEXUAL': 'BLOCK_NONE',
    'DANGEROUS': 'BLOCK_NONE'
}

model = genai.GenerativeModel(
model_name="gemini-1.0-pro",
generation_config=generation_config,
safety_settings=safety_settings,)

# Gemini uses 'model' for assistant; Streamlit uses 'assistant'
def role_to_streamlit(role):
  if role == "model":
    return "assistant"
  else:
    return role

# Add a Gemini Chat history object to Streamlit session state
if "chat" not in st.session_state:
    st.session_state.chat = model.start_chat(history = [
  {
    "role": "user",
    "parts": ["Como posso utilizar?"]
  },
  {
    "role": "model",
    "parts": ["Descreva, a necessidade, e se poss√≠vel a √°rea m√©dica!"]
  },
])

# T√≠tulo e Subt√≠tulo
st.title(":blue[tr] :violet[IA] :blue[gem] :heart: :hospital: üìç")
st.subheader("Informe em uma linha apenas, sua necessidade :memo:")

# Instru√ß√µes formatadas
st.markdown("""
<div style="background-color: #000; padding: 15px; border-radius: 10px;">
    <h4>Como utilizar:</h4>
    <ul>
      <li><strong>Inten√ß√£o: </strong> <em>Necessidade, problema de sa√∫de, urg√™ncia...</em></li>
      <li><strong>Setor da sa√∫de: </strong> <em>√Årea da sa√∫de, neurologia, cardiologia, psiquiatria...</em></li>
      <li><strong>Localidade: </strong> <em>Localidade mais pr√≥xima do posto de sa√∫de UBS ou Hospital (Cidade, Estado, Pa√≠s)...</em></li>
    </ul>
</div>
""", unsafe_allow_html=True)
st.markdown("<hr>", unsafe_allow_html=True)
st.divider()

# Display chat messages from history above current input box
for message in st.session_state.chat.history:
    with st.chat_message(role_to_streamlit(message.role)):
        st.markdown(message.parts[0].text)

# Accept user's next message, add to context, resubmit context to Gemini
if prompt := st.chat_input("Descreva sua necessidade e se poss√≠vel a √°rea m√©dica"):
    # Display user's last message
    st.chat_message("user").markdown(prompt)
    # Send user entry to Gemini and read the response
    response = st.session_state.chat.send_message(prompt) 
    
    # Display last 
    with st.chat_message("assistant"):
        st.markdown(response.text)